## 웹 크롤링해서 데이터 50개 다운받기
- pdf,image 가져오기
- backgroud가 그림인 것 위주
---

### **PubMed Central**
**크롤링 방법**
- keyword가 포함된 논문 리스트의 PMC ID를 각 키워드별로 상위 10개씩 가져와서 pmc_ids.txt파일로 저장
- 해당 Id를 PDF URL로 변경
- 페이지 HTML을 파싱하여 PDF 다운 태그 찾음
- PDF 다운받아서 로컬 폴더에 저장

1. 키워드 포함된 PMC ID 추출
    - NCBI Entrez API(eutils) 사용
    - esearch로 PMC ID(pmcid) 상위 10개 추출
    - XML/JSON 파싱 후 pmc_ids.txt에 저장
    
2. PMC ID → PDF URL 변환
    - 일반 패턴: https://www.ncbi.nlm.nih.gov/pmc/articles/PMCxxxxxx/pdf/
  
3. HTML 파싱
    - PDF 다운로드 버튼이나 <a> 태그 href 확인
    
4. PDF 다운로드
    - Open Access 여부 확인
    - HTTP 요청 헤더(Agent, Referer) 설정
    - 오류·재시도 로직 구현
  
#### pmc id 추출 코드
```python
import requests
import webbrowser
import time

keywords = ["health", "alzheimer", "life"]
esearch_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"

result_dict = {}

for keyword in keywords:
    params = {
        'db': 'pmc',
        'term': keyword,
        'retmax': 10,
        'retmode': 'json'
    }
    r = requests.get(esearch_url, params=params)
    ids = r.json()['esearchresult']['idlist']  # PMID/PMCID list
    result_dict[keyword] = ids

# pmc_ids.txt로 PMC ID 저장
with open('pmc_ids2.txt', 'w') as f:
    for keyword, pmcids in result_dict.items():
        f.write(f"Keyword: {keyword}\n")
        for pmcid in pmcids:
            f.write(pmcid + '\n')
        f.write("\n")

# pmc_urls.txt로 PDF URL 저장
with open('pmc_urls2.txt', 'w') as f:
    for keyword, pmcids in result_dict.items():
        f.write(f"Keyword: {keyword}\n")
        for pmcid in pmcids:
            url = f"https://pmc.ncbi.nlm.nih.gov/articles/PMC{pmcid}/pdf/"
            f.write(url + '\n')
        f.write("\n")


# pmc_urls.txt에서 URL 읽어서 순서대로 열기 
with open('pmc_urls2.txt', 'r') as f:
    lines = f.readlines()
    for line in lines:
        line = line.strip()
        if line.startswith("http"):  # URL만 골라서
            webbrowser.open(line)
            time.sleep(0.5) # 0.5초 간격
```

---


✍️ api를 이용하여 다운로드하려고 했는데 url구조가 ftp형식으로 chrome에서 지원이 안되는 문제, 해당 url 연결이 안되는 문제 발생 <br>
🚀 pmc id를 받아오는 코드 실행하여 id저장하는 txt파일, url로 변경하여 url도 txt 파일로 저장 후 url.txt파일 읽어서 웹 페이지로 열기 <br>
  macro 이용하여 pdf 다운받음 <br>
  중복되거나 부족한 부분은 수동 검토🥲 <br>

**💡keyword: death, life, alzheimer, ADHD, experiment, infection, corona**

---

### **data.gov**
**크롤링 방법**
- "report"로 데이터셋을 검색해 관련된 데이터셋 상세페이지 URL을 gov_url.txt에 저장
- gov_url.txt의 각 URL을 읽어와 하나씩 방문
- 페이지에서 모든 a 태그를 검사해서 .pdf로 끝나는 링크 탐색
- 찾은 PDF 파일들을 gov_pdfs 폴더에 다운로드

#### 코드
```python
import requests
from bs4 import BeautifulSoup
import os

# 1. 데이터셋 상세페이지 url을 가져와서 gov_url.txt로 저장
keyword = "international"
rows = 50  # 가져올 데이터셋 개수
search_url = f"https://catalog.data.gov/api/3/action/package_search?q={keyword}&rows={rows}"
response = requests.get(search_url)
result = response.json()

datasets = result["result"]["results"]

with open("gov_url.txt", "w") as f:
    for ds in datasets:
        dataset_url = "https://catalog.data.gov/dataset/" + ds["name"]
        f.write(dataset_url + "\n")

print(f"gov_url.txt에 {rows}개의 데이터셋 url 저장 완료.")

# 2. gov_url.txt의 각 url을 방문하여 pdf 파일 다운로드
if not os.path.exists("pdfs"):
    os.makedirs("pdfs")

with open("gov_url.txt", "r") as f:
    urls = f.read().splitlines()

for url in urls:
    print(f"페이지 방문: {url}")
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.content, "html.parser")
    except Exception as e:
        print(f"페이지 접근 실패: {e}")
        continue

    # 모든 a 태그에서 pdf 링크 찾기
    pdf_links = []
    for a in soup.find_all("a", href=True):
        href = a["href"]
        if href.lower().endswith(".pdf"):
            # 상대경로 처리
            if "://" not in href:
                pdf_url = "https://catalog.data.gov" + href
            else:
                pdf_url = href
            pdf_links.append(pdf_url)

    # pdf 다운로드
    for pdf_url in pdf_links:
        pdf_name = pdf_url.split("/")[-1]
        pdf_path = os.path.join("gov_pdfs", pdf_name)
        print(f"다운로드 중: {pdf_url}")
        try:
            pdf_response = requests.get(pdf_url)
            with open(pdf_path, "wb") as f:
                f.write(pdf_response.content)
            print(f"저장 완료: {pdf_name}")
        except Exception as e:
            print(f"다운로드 실패: {e}")
```

🤔🤔
- 주제를 바꿔가면서 50개씩 pdf를 저장시도함
- url 50개 중에서 pdf 다운로드가 안되는 게 대부분이고, 된다해도 페이지 수가 너무 많거나 pdf 파일을 열 수 없는데 대부분이었음
- 하나하나 확인하면서 데이터셋을 정제함🥲

<img width="800" height="600" alt="image" src="https://github.com/user-attachments/assets/ac350fa6-2079-4e22-b581-bd9b4743ae16" />

<img width="600" height="400" alt="image" src="https://github.com/user-attachments/assets/a184e8c6-d91c-40df-9849-f27c4d54c95b" />


---

### **EUR-Lex**

---

### **국회도서관**

---
