## ì›¹ í¬ë¡¤ë§í•´ì„œ ë°ì´í„° 50ê°œ ë‹¤ìš´ë°›ê¸°
- pdf,image ê°€ì ¸ì˜¤ê¸°
- backgroudê°€ ê·¸ë¦¼ì¸ ê²ƒ ìœ„ì£¼
---

### **PubMed Central**
**í¬ë¡¤ë§ ë°©ë²•**
- keywordê°€ í¬í•¨ëœ ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸ì˜ PMC IDë¥¼ ê° í‚¤ì›Œë“œë³„ë¡œ ìƒìœ„ 10ê°œì”© ê°€ì ¸ì™€ì„œ pmc_ids.txtíŒŒì¼ë¡œ ì €ì¥
- í•´ë‹¹ Idë¥¼ PDF URLë¡œ ë³€ê²½
- í˜ì´ì§€ HTMLì„ íŒŒì‹±í•˜ì—¬ PDF ë‹¤ìš´ íƒœê·¸ ì°¾ìŒ
- PDF ë‹¤ìš´ë°›ì•„ì„œ ë¡œì»¬ í´ë”ì— ì €ì¥

1. í‚¤ì›Œë“œ í¬í•¨ëœ PMC ID ì¶”ì¶œ
    - NCBI Entrez API(eutils) ì‚¬ìš©
    - esearchë¡œ PMC ID(pmcid) ìƒìœ„ 10ê°œ ì¶”ì¶œ
    - XML/JSON íŒŒì‹± í›„ pmc_ids.txtì— ì €ì¥
    
2. PMC ID â†’ PDF URL ë³€í™˜
    - ì¼ë°˜ íŒ¨í„´: https://www.ncbi.nlm.nih.gov/pmc/articles/PMCxxxxxx/pdf/
  
3. HTML íŒŒì‹±
    - PDF ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì´ë‚˜ <a> íƒœê·¸ href í™•ì¸
    
4. PDF ë‹¤ìš´ë¡œë“œ
    - Open Access ì—¬ë¶€ í™•ì¸
    - HTTP ìš”ì²­ í—¤ë”(Agent, Referer) ì„¤ì •
    - ì˜¤ë¥˜Â·ì¬ì‹œë„ ë¡œì§ êµ¬í˜„
  
#### pmc id ì¶”ì¶œ ì½”ë“œ
```python
import requests
import webbrowser
import time

keywords = ["health", "alzheimer", "life"]
esearch_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"

result_dict = {}

for keyword in keywords:
    params = {
        'db': 'pmc',
        'term': keyword,
        'retmax': 10,
        'retmode': 'json'
    }
    r = requests.get(esearch_url, params=params)
    ids = r.json()['esearchresult']['idlist']  # PMID/PMCID list
    result_dict[keyword] = ids

# pmc_ids.txtë¡œ PMC ID ì €ì¥
with open('pmc_ids2.txt', 'w') as f:
    for keyword, pmcids in result_dict.items():
        f.write(f"Keyword: {keyword}\n")
        for pmcid in pmcids:
            f.write(pmcid + '\n')
        f.write("\n")

# pmc_urls.txtë¡œ PDF URL ì €ì¥
with open('pmc_urls2.txt', 'w') as f:
    for keyword, pmcids in result_dict.items():
        f.write(f"Keyword: {keyword}\n")
        for pmcid in pmcids:
            url = f"https://pmc.ncbi.nlm.nih.gov/articles/PMC{pmcid}/pdf/"
            f.write(url + '\n')
        f.write("\n")


# pmc_urls.txtì—ì„œ URL ì½ì–´ì„œ ìˆœì„œëŒ€ë¡œ ì—´ê¸° 
with open('pmc_urls2.txt', 'r') as f:
    lines = f.readlines()
    for line in lines:
        line = line.strip()
        if line.startswith("http"):  # URLë§Œ ê³¨ë¼ì„œ
            webbrowser.open(line)
            time.sleep(0.5) # 0.5ì´ˆ ê°„ê²©
```

---


âœï¸ apië¥¼ ì´ìš©í•˜ì—¬ ë‹¤ìš´ë¡œë“œí•˜ë ¤ê³  í–ˆëŠ”ë° urlêµ¬ì¡°ê°€ ftpí˜•ì‹ìœ¼ë¡œ chromeì—ì„œ ì§€ì›ì´ ì•ˆë˜ëŠ” ë¬¸ì œ, í•´ë‹¹ url ì—°ê²°ì´ ì•ˆë˜ëŠ” ë¬¸ì œ ë°œìƒ <br>
ğŸš€ pmc idë¥¼ ë°›ì•„ì˜¤ëŠ” ì½”ë“œ ì‹¤í–‰í•˜ì—¬ idì €ì¥í•˜ëŠ” txtíŒŒì¼, urlë¡œ ë³€ê²½í•˜ì—¬ urlë„ txt íŒŒì¼ë¡œ ì €ì¥ í›„ url.txtíŒŒì¼ ì½ì–´ì„œ ì›¹ í˜ì´ì§€ë¡œ ì—´ê¸° <br>
  macro ì´ìš©í•˜ì—¬ pdf ë‹¤ìš´ë°›ìŒ <br>
  ì¤‘ë³µë˜ê±°ë‚˜ ë¶€ì¡±í•œ ë¶€ë¶„ì€ ìˆ˜ë™ ê²€í† ğŸ¥² <br>

**ğŸ’¡keyword: death, life, alzheimer, ADHD, experiment, infection, corona**

---

### **data.gov**
**í¬ë¡¤ë§ ë°©ë²•**
- "report"ë¡œ ë°ì´í„°ì…‹ì„ ê²€ìƒ‰í•´ ê´€ë ¨ëœ ë°ì´í„°ì…‹ ìƒì„¸í˜ì´ì§€ URLì„ gov_url.txtì— ì €ì¥
- gov_url.txtì˜ ê° URLì„ ì½ì–´ì™€ í•˜ë‚˜ì”© ë°©ë¬¸
- í˜ì´ì§€ì—ì„œ ëª¨ë“  a íƒœê·¸ë¥¼ ê²€ì‚¬í•´ì„œ .pdfë¡œ ëë‚˜ëŠ” ë§í¬ íƒìƒ‰
- ì°¾ì€ PDF íŒŒì¼ë“¤ì„ gov_pdfs í´ë”ì— ë‹¤ìš´ë¡œë“œ

#### ì½”ë“œ
```python
import requests
from bs4 import BeautifulSoup
import os

# 1. ë°ì´í„°ì…‹ ìƒì„¸í˜ì´ì§€ urlì„ ê°€ì ¸ì™€ì„œ gov_url.txtë¡œ ì €ì¥
keyword = "international"
rows = 50  # ê°€ì ¸ì˜¬ ë°ì´í„°ì…‹ ê°œìˆ˜
search_url = f"https://catalog.data.gov/api/3/action/package_search?q={keyword}&rows={rows}"
response = requests.get(search_url)
result = response.json()

datasets = result["result"]["results"]

with open("gov_url.txt", "w") as f:
    for ds in datasets:
        dataset_url = "https://catalog.data.gov/dataset/" + ds["name"]
        f.write(dataset_url + "\n")

print(f"gov_url.txtì— {rows}ê°œì˜ ë°ì´í„°ì…‹ url ì €ì¥ ì™„ë£Œ.")

# 2. gov_url.txtì˜ ê° urlì„ ë°©ë¬¸í•˜ì—¬ pdf íŒŒì¼ ë‹¤ìš´ë¡œë“œ
if not os.path.exists("pdfs"):
    os.makedirs("pdfs")

with open("gov_url.txt", "r") as f:
    urls = f.read().splitlines()

for url in urls:
    print(f"í˜ì´ì§€ ë°©ë¬¸: {url}")
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.content, "html.parser")
    except Exception as e:
        print(f"í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
        continue

    # ëª¨ë“  a íƒœê·¸ì—ì„œ pdf ë§í¬ ì°¾ê¸°
    pdf_links = []
    for a in soup.find_all("a", href=True):
        href = a["href"]
        if href.lower().endswith(".pdf"):
            # ìƒëŒ€ê²½ë¡œ ì²˜ë¦¬
            if "://" not in href:
                pdf_url = "https://catalog.data.gov" + href
            else:
                pdf_url = href
            pdf_links.append(pdf_url)

    # pdf ë‹¤ìš´ë¡œë“œ
    for pdf_url in pdf_links:
        pdf_name = pdf_url.split("/")[-1]
        pdf_path = os.path.join("gov_pdfs", pdf_name)
        print(f"ë‹¤ìš´ë¡œë“œ ì¤‘: {pdf_url}")
        try:
            pdf_response = requests.get(pdf_url)
            with open(pdf_path, "wb") as f:
                f.write(pdf_response.content)
            print(f"ì €ì¥ ì™„ë£Œ: {pdf_name}")
        except Exception as e:
            print(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
```

ğŸ¤”ğŸ¤”
- ì£¼ì œë¥¼ ë°”ê¿”ê°€ë©´ì„œ 50ê°œì”© pdfë¥¼ ì €ì¥ì‹œë„í•¨
- url 50ê°œ ì¤‘ì—ì„œ pdf ë‹¤ìš´ë¡œë“œê°€ ì•ˆë˜ëŠ” ê²Œ ëŒ€ë¶€ë¶„ì´ê³ , ëœë‹¤í•´ë„ í˜ì´ì§€ ìˆ˜ê°€ ë„ˆë¬´ ë§ê±°ë‚˜ pdf íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ëŠ”ë° ëŒ€ë¶€ë¶„ì´ì—ˆìŒ
- í•˜ë‚˜í•˜ë‚˜ í™•ì¸í•˜ë©´ì„œ ë°ì´í„°ì…‹ì„ ì •ì œí•¨ğŸ¥²

<img width="800" height="600" alt="image" src="https://github.com/user-attachments/assets/ac350fa6-2079-4e22-b581-bd9b4743ae16" />

<img width="600" height="400" alt="image" src="https://github.com/user-attachments/assets/a184e8c6-d91c-40df-9849-f27c4d54c95b" />


---

### **EUR-Lex**

---

### **êµ­íšŒë„ì„œê´€**

---
