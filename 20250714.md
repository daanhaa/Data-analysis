### EUR-Lex

크롤링
1. 키워드 검색한 url 입력
2. 크롤링 코드로 url 링크 txt파일로 저장
3. 해당 링크의 pdf 다운

➖파일명이 원본 명으로 저장되지 않는 문제 발생
```python3
import requests
from bs4 import BeautifulSoup
import os
from urllib.parse import urlparse, unquote

BASE_URL = "https://eur-lex.europa.eu"
SEARCH_URL = "https://eur-lex.europa.eu/search.html?scope=EURLEX&text=legal&lang=en&type=quick&qid=1752456970660"

print("1단계: 검색 결과 페이지 요청 중...")
response = requests.get(SEARCH_URL)
if response.status_code != 200:
    print("검색 페이지 요청 실패:", response.status_code)
    exit()
soup = BeautifulSoup(response.text, 'html.parser')
print("1단계 완료: 페이지 요청 성공")

print("2단계: 영어 PDF 링크와 제목 찾는 중...")
pdf_infos = []
for a in soup.find_all('a', href=True, class_="piwik_download"):
    href = a['href']

    # 링크 내 PDF 여부
    span = a.find('span', class_="visible-print-block")
    if span and span.text.strip().lower() == 'pdf' and 'pdf' in href.lower():
        if not href.startswith("http"):
            href = BASE_URL + href.replace("./", "/")
        if '/EN/' in href:  # 영어 PDF만
            # 1) 제목 탐색 (부모 div .result 등에서 title 추출)
            title = None
            parent = a.find_parent(class_="result")
            if parent:
                title_tag = parent.find('a', class_="title")
                if title_tag:
                    title = title_tag.get_text(strip=True)
            # 만약 title 못 얻으면 임시 파일명
            if not title:
                title = "document_" + str(len(pdf_infos)+1)
            pdf_infos.append( (href, title) )

print(f"2단계 완료: 총 {len(pdf_infos)}개의 영어 PDF 발견")

save_folder = "EUR_pdfs"
if not os.path.exists(save_folder):
    os.makedirs(save_folder)

for i, (link, title) in enumerate(pdf_infos):
    # 파일명 안전하게 만들기
    safe_title = "".join(c if c.isalnum() or c in " ._-" else "_" for c in title)
    file_path = os.path.join(save_folder, f"{safe_title}.pdf")
    print(f"{i+1}번째 PDF 다운로드 중: {link} -> {file_path}")
    try:
        pdf_res = requests.get(link)
        if pdf_res.status_code == 200:
            with open(file_path, "wb") as f:
                f.write(pdf_res.content)
            print(f"{file_path} 저장 완료")
        else:
            print(f"{link} 다운로드 실패: {pdf_res.status_code}")
    except Exception as e:
        print(f"{link} 처리 중 예외 발생:", e)

print("모든 단계 완료")
```
<img width="659" height="357" alt="image" src="https://github.com/user-attachments/assets/b6a334d4-b57f-4130-a91b-2bb71a76c7b7" />

<img width="1615" height="271" alt="image" src="https://github.com/user-attachments/assets/865fa421-f004-4c09-ab96-cc9fd742ada4" />
다운로드 과정에서 pdf 명을 바꾸지 말고 고유 번호로 파일명 지정하여 다운 성공

```python3
import requests
from bs4 import BeautifulSoup
import os
from urllib.parse import urlparse, parse_qs

BASE_URL = "https://eur-lex.europa.eu"
SEARCH_URL = "https://eur-lex.europa.eu/search.html?scope=EURLEX&text=information&lang=en&type=quick&qid=1752459016049"

response = requests.get(SEARCH_URL)
if response.status_code != 200:
    print("검색 페이지 요청 실패:", response.status_code)
    exit()
soup = BeautifulSoup(response.text, 'html.parser')

pdf_infos = []
for a in soup.find_all('a', href=True, class_="piwik_download"):
    href = a['href']
    span = a.find('span', class_="visible-print-block")
    if span and span.text.strip().lower() == 'pdf' and 'pdf' in href.lower():
        if not href.startswith("http"):
            href = BASE_URL + href.replace("./", "/")
        if '/EN/' in href:  # 영어 PDF만
            # CELEX 번호 추출, 기본값은 None
            celex_id = None
            parsed = urlparse(href)
            query = parse_qs(parsed.query)
            if 'uri' in query and query['uri'][0].startswith("CELEX:"):
                celex_id = query['uri'][0]
            # 만약 CELEX 번호 못 찾으면 기존 방식 유지
            if celex_id:
                title = celex_id
            else:
                title = "document_" + str(len(pdf_infos)+1)
            pdf_infos.append((href, title))

save_folder = "EUR_pdfs"
if not os.path.exists(save_folder):
    os.makedirs(save_folder)

for i, (link, title) in enumerate(pdf_infos):
    # CELEX:..... → CELEX_..... 으로 변환
    safe_title = title.replace(":", "_")
    file_path = os.path.join(save_folder, f"{safe_title}.pdf")
    print(f"{i+1}번째 PDF 다운로드 중: {link} -> {file_path}")
    try:
        pdf_res = requests.get(link)
        if pdf_res.status_code == 200:
            with open(file_path, "wb") as f:
                f.write(pdf_res.content)
            print(f"{file_path} 저장 완료")
        else:
            print(f"{link} 다운로드 실패: {pdf_res.status_code}")
    except Exception as e:
        print(f"{link} 처리 중 예외 발생:", e)
```


---


