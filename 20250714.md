### EUR-Lex

**크롤링**
1. 키워드 검색한 url 입력
2. 크롤링 코드로 url 링크 txt파일로 저장
3. 해당 링크의 pdf 다운

<br>

➖파일명이 원본 명으로 저장되지 않는 문제 발생
<br>

```python3
import requests
from bs4 import BeautifulSoup
import os
from urllib.parse import urlparse, unquote

BASE_URL = "https://eur-lex.europa.eu"
SEARCH_URL = "https://eur-lex.europa.eu/search.html?scope=EURLEX&text=legal&lang=en&type=quick&qid=1752456970660"

print("1단계: 검색 결과 페이지 요청 중...")
response = requests.get(SEARCH_URL)
if response.status_code != 200:
    print("검색 페이지 요청 실패:", response.status_code)
    exit()
soup = BeautifulSoup(response.text, 'html.parser')
print("1단계 완료: 페이지 요청 성공")

print("2단계: 영어 PDF 링크와 제목 찾는 중...")
pdf_infos = []
for a in soup.find_all('a', href=True, class_="piwik_download"):
    href = a['href']

    # 링크 내 PDF 여부
    span = a.find('span', class_="visible-print-block")
    if span and span.text.strip().lower() == 'pdf' and 'pdf' in href.lower():
        if not href.startswith("http"):
            href = BASE_URL + href.replace("./", "/")
        if '/EN/' in href:  # 영어 PDF만
            # 1) 제목 탐색 (부모 div .result 등에서 title 추출)
            title = None
            parent = a.find_parent(class_="result")
            if parent:
                title_tag = parent.find('a', class_="title")
                if title_tag:
                    title = title_tag.get_text(strip=True)
            # 만약 title 못 얻으면 임시 파일명
            if not title:
                title = "document_" + str(len(pdf_infos)+1)
            pdf_infos.append( (href, title) )

print(f"2단계 완료: 총 {len(pdf_infos)}개의 영어 PDF 발견")

save_folder = "EUR_pdfs"
if not os.path.exists(save_folder):
    os.makedirs(save_folder)

for i, (link, title) in enumerate(pdf_infos):
    # 파일명 안전하게 만들기
    safe_title = "".join(c if c.isalnum() or c in " ._-" else "_" for c in title)
    file_path = os.path.join(save_folder, f"{safe_title}.pdf")
    print(f"{i+1}번째 PDF 다운로드 중: {link} -> {file_path}")
    try:
        pdf_res = requests.get(link)
        if pdf_res.status_code == 200:
            with open(file_path, "wb") as f:
                f.write(pdf_res.content)
            print(f"{file_path} 저장 완료")
        else:
            print(f"{link} 다운로드 실패: {pdf_res.status_code}")
    except Exception as e:
        print(f"{link} 처리 중 예외 발생:", e)

print("모든 단계 완료")
```
<img width="659" height="357" alt="image" src="https://github.com/user-attachments/assets/b6a334d4-b57f-4130-a91b-2bb71a76c7b7" />

<br>

<img width="1615" height="271" alt="image" src="https://github.com/user-attachments/assets/865fa421-f004-4c09-ab96-cc9fd742ada4" />

<br>

다운로드 과정에서 pdf 명을 바꾸지 말고 고유 번호로 파일명 지정하여 다운 성공

<br>

```python3
import requests
from bs4 import BeautifulSoup
import os
from urllib.parse import urlparse, parse_qs

BASE_URL = "https://eur-lex.europa.eu"
SEARCH_URL = "https://eur-lex.europa.eu/search.html?scope=EURLEX&text=information&lang=en&type=quick&qid=1752459016049"

response = requests.get(SEARCH_URL)
if response.status_code != 200:
    print("검색 페이지 요청 실패:", response.status_code)
    exit()
soup = BeautifulSoup(response.text, 'html.parser')

pdf_infos = []
for a in soup.find_all('a', href=True, class_="piwik_download"):
    href = a['href']
    span = a.find('span', class_="visible-print-block")
    if span and span.text.strip().lower() == 'pdf' and 'pdf' in href.lower():
        if not href.startswith("http"):
            href = BASE_URL + href.replace("./", "/")
        if '/EN/' in href:  # 영어 PDF만
            # CELEX 번호 추출, 기본값은 None
            celex_id = None
            parsed = urlparse(href)
            query = parse_qs(parsed.query)
            if 'uri' in query and query['uri'][0].startswith("CELEX:"):
                celex_id = query['uri'][0]
            # 만약 CELEX 번호 못 찾으면 기존 방식 유지
            if celex_id:
                title = celex_id
            else:
                title = "document_" + str(len(pdf_infos)+1)
            pdf_infos.append((href, title))

save_folder = "EUR_pdfs"
if not os.path.exists(save_folder):
    os.makedirs(save_folder)

for i, (link, title) in enumerate(pdf_infos):
    # CELEX:..... → CELEX_..... 으로 변환
    safe_title = title.replace(":", "_")
    file_path = os.path.join(save_folder, f"{safe_title}.pdf")
    print(f"{i+1}번째 PDF 다운로드 중: {link} -> {file_path}")
    try:
        pdf_res = requests.get(link)
        if pdf_res.status_code == 200:
            with open(file_path, "wb") as f:
                f.write(pdf_res.content)
            print(f"{file_path} 저장 완료")
        else:
            print(f"{link} 다운로드 실패: {pdf_res.status_code}")
    except Exception as e:
        print(f"{link} 처리 중 예외 발생:", e)
```
<br>

법률 문서로 거의 유사한 형태의 자료
<img width="2536" height="1361" alt="image" src="https://github.com/user-attachments/assets/bf7a4e50-04d1-4107-8b36-7c2d9f3d8f80" />

<br>

---
<br>

### Internet Archive

**크롤링 방법**
- API 활용
- 카테고리 입력하여 자료 다운로드
  <img width="1257" height="363" alt="image" src="https://github.com/user-attachments/assets/b288d147-1206-473d-a737-34ee3c24e30a" />

<br>

```python3
import requests
import os
from PyPDF2 import PdfReader

root_url = "https://archive.org"
search_api = "https://archive.org/advancedsearch.php"
download_dir = "Internet_Archive_pdfs"
os.makedirs(download_dir, exist_ok=True)

# 결과 url 저장용
txt_save_path = "book_urls.txt"
book_urls = []

def get_book_list(page=1, rows=30):
    params = {
        "q": '(collection: smithsonian) AND (language:"English") AND (format:"PDF")',
        "fl[]": 'identifier',
        "rows": rows,
        "page": page,
        "output": "json"
    }
    res = requests.get(search_api, params=params)
    res.raise_for_status()
    data = res.json()
    docs = data['response']['docs']
    return docs, data['response']['numFound']

def download_and_check_pdf(identifier):
    item_url = f"{root_url}/details/{identifier}"
    # 파일 리스트 api
    files_api = f"https://archive.org/metadata/{identifier}"
    res = requests.get(files_api)
    files = res.json()['files']
    for file in files:
        if file.get('format') == 'Text PDF':
            pdf_url = f"https://archive.org/download/{identifier}/{file['name']}"
            # PDF 다운로드
            pdf_res = requests.get(pdf_url)
            temp_pdf = "temp.pdf"
            with open(temp_pdf, "wb") as f:
                f.write(pdf_res.content)

            try:
                reader = PdfReader(temp_pdf)
                num_pages = len(reader.pages)
                if num_pages <= 50:
                    # 저장
                    file_name = os.path.join(download_dir, file['name'])
                    with open(file_name, "wb") as f:
                        f.write(pdf_res.content)
                    print(f"다운로드: {file_name} ({num_pages} pages)")
                    # url 기록
                    book_urls.append(item_url)
                else:
                    print(f"페이지 초과: {item_url} ({num_pages} page)")
            except Exception as e:
                print(f"PDF 오류: {item_url}", e)
            finally:
                os.remove(temp_pdf)
            break # 한 책에 PDF 하나만

# 메인 루프
page = 1
rows = 30   
while True:
    books, num_found = get_book_list(page, rows)
    if not books:
        break
    for book in books:
        download_and_check_pdf(book['identifier'])
    if (page * rows) >= num_found:
        break
    page += 1

# url txt로 저장
with open(txt_save_path, "w", encoding="utf-8") as f:
    for url in book_urls:
        f.write(url + '\n')

print(f"완료! URL들은 {txt_save_path}에 저장됨")
```

<br>

다양한 카테고리, 다양한 형태의 자료 다운로드
<img width="2559" height="1385" alt="image" src="https://github.com/user-attachments/assets/b18e9957-f15e-4fa9-8599-de9eca256f58" />


--- 

**➖ 국회도서관, MIT OpenCourseWare 사이트 크롤링 실패**
**국회도서관**
- 다운로드 버튼 위치를 F12로 찾아냈지만 코드에서 적용 실패함.
- <img width="600" height="521" alt="image" src="https://github.com/user-attachments/assets/a6de82af-ab81-41a5-8e0f-5ffa4c14d144" />

<br>

**MIT OpenCourseWare**
- 키워드 검색, 강의 접속 후 Download Course에서 pdf를 다운받아야 하는 복잡한 구조로 코드 적용 실패함
<img width="700" height="588" alt="image" src="https://github.com/user-attachments/assets/51d30d4d-568b-4035-977e-3c9fa1587606" />
